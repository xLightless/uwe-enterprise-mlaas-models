{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defc5e02",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Subject: Advanced Artificial Intelligence, 1st October 2024 to 1st May 2025.](#toc0_)\n",
    "## <a id='toc1_1_'></a>[Topic: Group Project - Developing the MARS Model](#toc0_)\n",
    "## <a id='toc1_2_'></a>[Learning Outcomes](#toc0_)\n",
    "1. Critically appraise competing AI-based paradigms and algorithms in the context of the issues posed by particular problems\n",
    "2. Select and tune appropriate state of the art machine learning and optimisation algorithms to realistic sized problems illustrating properties such as scale, noise, missing data.\n",
    "3. Synthesise, design, and implement appropriate hybrid systems blending different paradigms for complex problems.\n",
    "4. Demonstrate skills in evaluating systems and presenting findings in ways appropriate to different audiences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d754f20",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Subject: Advanced Artificial Intelligence, 1st October 2024 to 1st May 2025.](#toc1_)    \n",
    "  - [Topic: Group Project - Developing the MARS Model](#toc1_1_)    \n",
    "  - [Learning Outcomes:](#toc1_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48967f",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe48727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "PATH_TO_UWE_ENTERPRISE_MLAAS_MODELS = \"C:/Program Development/UWE Bristol/Year 3/Enterprise AI Project/project/backend/services/machinelearning/uwe_enterprise_mlaas_models\"\n",
    "sys.path.append(PATH_TO_UWE_ENTERPRISE_MLAAS_MODELS)\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc17d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from models import (\n",
    "        DataPreprocessor,\n",
    "        DataStandardizer,\n",
    "        Encoder,\n",
    "        Scaler,\n",
    "        Imputer,\n",
    "        RANDOM_STATE,\n",
    "        TESTING_DATA_SIZE,\n",
    "        TARGET_VARIABLE_COL_NUM,\n",
    "        datasets_processed_directory,\n",
    "        datasets_raw_directory,\n",
    "        insurance_dataset,\n",
    "        gdpr_protected_cols,\n",
    "        invalid_cols,\n",
    "        medical_protected_cols,\n",
    "        datetime_cols,\n",
    "\n",
    "        # Models\n",
    "        MARS\n",
    "    )\n",
    "except ImportError:\n",
    "    from models import (\n",
    "        DataPreprocessor,\n",
    "        DataStandardizer,\n",
    "        Encoder,\n",
    "        Scaler,\n",
    "        Imputer,\n",
    "        RANDOM_STATE,\n",
    "        TESTING_DATA_SIZE,\n",
    "        TARGET_VARIABLE_COL_NUM,\n",
    "        datasets_processed_directory,\n",
    "        datasets_raw_directory,\n",
    "        insurance_dataset,\n",
    "        gdpr_protected_cols,\n",
    "        invalid_cols,\n",
    "        medical_protected_cols,\n",
    "        datetime_cols,\n",
    "\n",
    "        # Models\n",
    "        MARS\n",
    "    )\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5372b18",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b079d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path:  c:\\Program Development\\UWE Bristol\\Year 3\\Enterprise AI Project\\project\\backend\\services\\machinelearning\\uwe-enterprise-mlaas-models\\datasets\\raw\\insurance.csv\n",
      "Data Shape:  (5000, 36)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Path: \", insurance_dataset)\n",
    "\n",
    "data = pd.read_csv(\n",
    "    insurance_dataset\n",
    ")\n",
    "\n",
    "print(\"Data Shape: \", data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1b07b",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset\n",
    "In this section the raw dataset is passed into a preprocessor, which uses internal functions for refactoring columns names. Moreover, each column has been imputed and encoded or scaled depending on the type of data and its use case.\n",
    "\n",
    "```python\n",
    "class DataPreprocessor(\n",
    "    df: DataFrame,\n",
    "    target_variable: str = None,\n",
    "    protected_cols: list[str] = None\n",
    ")\n",
    "```\n",
    "\n",
    "- The data preprocessor class automatically handles the preprocessing of linear/tabular data in the form of numerical, datetime, and categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f27bdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Missing values found. If populating value is 0, this does not always mean the existing columns are filled.\n",
      "[INFO] Imputed 2640 numerical, 247 datetime, 1225 categorical values.\n",
      "[INFO] Found 2 datetime columns: ['AccidentDate', 'ClaimDate']\n",
      "[INFO] Found 4459 rows in 'InjuryPrognosis' that do not match the expected structure.\n",
      "Encoding cyclical features: ['AccidentDateYear', 'AccidentDateMonth', 'AccidentDateDay', 'AccidentDateHour', 'ClaimDateYear', 'ClaimDateMonth', 'ClaimDateDay', 'ClaimDateHour']\n",
      "Encoding cyclical features: ['PrognosisEndDateYear', 'PrognosisEndDateMonth', 'PrognosisEndDateDay', 'PrognosisEndDateHour']\n",
      "0       0.054890\n",
      "1       0.259481\n",
      "2       0.823353\n",
      "3       0.054890\n",
      "4       0.002994\n",
      "          ...   \n",
      "4454    0.009980\n",
      "4455    0.407186\n",
      "4456    0.802395\n",
      "4457    0.195609\n",
      "4458    0.578842\n",
      "Name: SettlementValue, Length: 4459, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Development\\UWE Bristol\\Year 3\\Enterprise AI Project\\project\\backend\\services\\machinelearning\\uwe-enterprise-mlaas-models\\models\\preprocessing\\preprocessor.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'AccidentClaimDeltaInDays'] = (\n"
     ]
    }
   ],
   "source": [
    "\n",
    "protected_cols = (\n",
    "    gdpr_protected_cols + medical_protected_cols + datetime_cols + invalid_cols\n",
    ")\n",
    "\n",
    "TARGET_VARIABLE = \"SettlementValue\"\n",
    "\n",
    "# target_variable_col = data[TARGET_VARIABLE]\n",
    "\n",
    "processor = DataPreprocessor(\n",
    "    df=data,\n",
    "    # target_variable=TARGET_VARIABLE,\n",
    "    protected_cols=protected_cols\n",
    ")\n",
    "\n",
    "# processor.df[TARGET_VARIABLE] = target_variable_col\n",
    "target = processor.df[TARGET_VARIABLE].copy()\n",
    "\n",
    "if (target.isnull().sum() > 0):\n",
    "    print(\"Target variable has null values\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb29a2a",
   "metadata": {},
   "source": [
    "## Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e479bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame Shape:  (1500, 56)\n",
      "X_train Shape:  (1200, 55)\n",
      "y_train Shape:  (1200,)\n",
      "X_test Shape:  (300, 55)\n",
      "y_test Shape:  (300,)\n"
     ]
    }
   ],
   "source": [
    "df = processor.df.copy()\n",
    "\n",
    "df = df.iloc[: 1500]  # Reduce the dataset size for time complexity\n",
    "print(\"Data Frame Shape: \", df.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=[TARGET_VARIABLE]),\n",
    "    df[TARGET_VARIABLE],\n",
    "    test_size=TESTING_DATA_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"X_train Shape: \", X_train.shape)\n",
    "print(\"y_train Shape: \", y_train.shape)\n",
    "print(\"X_test Shape: \", X_test.shape)\n",
    "print(\"y_test Shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db21c9",
   "metadata": {},
   "source": [
    "## Check for missing data in the dataset before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a37303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(X, y):\n",
    "    \"\"\"\n",
    "    Checks for NaN or infinite values in X and y.\n",
    "\n",
    "    Parameters:\n",
    "        X (DataFrame): Features.\n",
    "        y (Series): Target.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if NaN or infinite values are found, False otherwise.\n",
    "    \"\"\"\n",
    "    if X.isna().any().any() or y.isna().any():\n",
    "        print(\"NaN values found in X or y.\")\n",
    "        return True\n",
    "    if X.isin([float('inf'), float('-inf')]).any().any() or y.isin([\n",
    "        float('inf'), float('-inf')\n",
    "    ]).any():\n",
    "        print(\"Infinite values found in X or y.\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "missing_values = check_missing_values(X_train, y_train) or check_missing_values(X_test, y_test)\n",
    "if missing_values:\n",
    "    print(\"Missing values detected in the dataset.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb41d5",
   "metadata": {},
   "source": [
    "# Training the MARS model\n",
    "The Multivariate Adaptive Regression Splines (MARS)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300840fe",
   "metadata": {},
   "source": [
    "## Grid Search - fit function and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f080245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, n_chunks):\n",
    "    \"\"\"\n",
    "    Split the data into chunks (processors) for parallel processing.\n",
    "    \"\"\"\n",
    "    X = np.atleast_2d(X)\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    chunk_size = len(X) // n_chunks\n",
    "    return [(\n",
    "            X[i * chunk_size:(i + 1) * chunk_size],\n",
    "            y[i * chunk_size:(i + 1) * chunk_size]\n",
    "            ) for i in range(n_chunks)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2837c",
   "metadata": {},
   "source": [
    "### Grid Search and Model trained on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def predict(models, X):\n",
    "    \"\"\"\n",
    "    Predict using the ensemble of models.\n",
    "\n",
    "    Rather than averaging normally, the model can apply weights\n",
    "    based on individual model performance, thus models with\n",
    "    better performance should have a higher weight which improves prediction.\n",
    "    \"\"\"\n",
    "    chunked_predictions = np.array([model.predict(X) for model in models])\n",
    "    return np.mean(chunked_predictions, axis=0)\n",
    "\n",
    "def weighted_mse(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Calculate weighted mean squared error.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (ndarray): True target values.\n",
    "        y_pred (ndarray): Predicted target values.\n",
    "        weights (ndarray): Weights for each prediction.\n",
    "\n",
    "    Returns:\n",
    "        float: Weighted mean squared error.\n",
    "    \"\"\"\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    weighted_mse = np.sum(weights * squared_errors) / np.sum(weights)\n",
    "    return weighted_mse\n",
    "\n",
    "def weighted_r2(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Calculate weighted R^2 score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (ndarray): True target values.\n",
    "        y_pred (ndarray): Predicted target values.\n",
    "        weights (ndarray): Weights for each prediction.\n",
    "\n",
    "    Returns:\n",
    "        float: Weighted R^2 score.\n",
    "    \"\"\"\n",
    "    ss_res = ((y_true - y_pred) ** 2 * weights).sum()\n",
    "    ss_tot = ((y_true - y_true.mean()) ** 2 * weights).sum()\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "\n",
    "def evaluate_ensemble(models, X_test, y_test, weights):\n",
    "    \"\"\"\n",
    "    Evaluate the ensemble of models using weighted MSE and weighted R^2.\n",
    "    \"\"\"\n",
    "    preds = np.mean([model.predict(X_test) for model in models], axis=0)\n",
    "    mse = weighted_mse(y_test, preds, weights)\n",
    "    r2 = weighted_r2(y_test, preds, weights)\n",
    "    return mse, r2\n",
    "\n",
    "def grid_search(model_class: MARS, fit_function, X_train, y_train, X_test, y_test, param_grid, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Custom grid search with parallel processing for hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (DataFrame): Training features.\n",
    "        y_train (Series): Training target variable.\n",
    "        X_test (DataFrame): Testing features.\n",
    "        y_test (Series): Testing target variable.\n",
    "        param_grid (dict): Dictionary of hyperparameters to search.\n",
    "        n_jobs (int): Number of parallel jobs. Default is -1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Best parameters, their corresponding score, and the best model.\n",
    "    \"\"\"\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [\n",
    "        dict(zip(keys, v))\n",
    "        for v in itertools.product(*values)\n",
    "    ]\n",
    "\n",
    "    n_chunks = n_jobs\n",
    "    data_chunks = split_data(X_train, y_train, n_chunks)\n",
    "    print(f\"Data chunks {len(data_chunks)}: \", [\n",
    "        len(chunk) for chunk in data_chunks\n",
    "    ])\n",
    "\n",
    "    best_params = None\n",
    "    avg_best_r2 = float('-inf')\n",
    "    avg_best_mse = float('inf')\n",
    "    best_ensemble = None\n",
    "    grid_searches = []\n",
    "    best_i = 0\n",
    "\n",
    "    print(f\"Combinations: {len(param_combinations)}\")\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        weights = np.ones(len(y_test))\n",
    "        models = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(fit_function)(model_class, X_chunk, y_chunk, **params)\n",
    "            for X_chunk, y_chunk in data_chunks\n",
    "        )\n",
    "\n",
    "        models = [model for model in models if model is not None]\n",
    "        if not models:\n",
    "            print(f\"No valid models for parameters: {params}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        avg_mse, avg_r2 = evaluate_ensemble(models, X_test, y_test, weights)\n",
    "        print(\n",
    "            f\"{i}: {models[0]} - \" +\n",
    "            f\"Weighted MSE: {avg_mse}, Weighted R^2: {avg_r2}\"\n",
    "        )\n",
    "\n",
    "        grid_searches.append({\n",
    "            'params': params,\n",
    "            'w_mse': avg_mse,\n",
    "            'w_r2': avg_r2\n",
    "        })\n",
    "\n",
    "        if avg_r2 > avg_best_r2 and avg_mse < avg_best_mse:\n",
    "            avg_best_r2 = avg_r2\n",
    "            avg_best_mse = avg_mse\n",
    "            best_params = params\n",
    "            best_ensemble = models\n",
    "            best_i = i\n",
    "\n",
    "    print(\n",
    "        f\"\\n[{best_i}] Best Params: {best_params}\" +\n",
    "        f\"\\n Weighted MSE: {avg_best_mse}\\nWeighted R^2: {avg_best_r2}\"\n",
    "    )\n",
    "    return best_ensemble, np.array(grid_searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4544a819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search...\n",
      "Data chunks 10:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Combinations: 1\n",
      "0: MARS(max_degree=3, max_terms=100, min_samples_split=4, penalty=0.1) - MSE: 0.023470997242153555, R^2: 0.7311066059994888\n",
      "\n",
      "[0] Best Params: {'max_terms': 100, 'max_degree': 3, 'min_samples_split': 4, 'penalty': 0.1}\n",
      "Ensemble Averages: \n",
      "MSE: 0.023470997242153555\n",
      "R^2: 0.7311066059994888\n"
     ]
    }
   ],
   "source": [
    "from models.types.experimental.mars.mars import MARS\n",
    "\n",
    "# param_grid = {\n",
    "#     \"max_terms\": [400],\n",
    "#     \"max_degree\": [2],\n",
    "#     \"min_samples_split\": [4],\n",
    "#     \"penalty\": [0.1]\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'max_terms': [100],\n",
    "    'max_degree': [3],\n",
    "    'min_samples_split': [4],\n",
    "    'penalty': [0.1]\n",
    "}\n",
    "\n",
    "def fit_mars_model(model_class: MARS, X, y, max_terms, max_degree, min_samples_split, penalty):\n",
    "    X = np.atleast_2d(X)\n",
    "    y = np.ravel(y)\n",
    "    model = model_class(\n",
    "        max_terms=max_terms,\n",
    "        max_degree=max_degree,\n",
    "        min_samples_split=min_samples_split,\n",
    "        penalty=penalty,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        model.fit(X, y)\n",
    "        print(f\"Fitted model with params: {max_terms}, {max_degree}, {min_samples_split}, {penalty}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model with params: {max_terms}, {max_degree}, {min_samples_split}, {penalty}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    return model\n",
    "\n",
    "if max(param_grid['max_terms']) <= df.shape[0]:\n",
    "    print(\"Starting grid search...\")\n",
    "    best_ensemble, grid_searches = grid_search(\n",
    "        model_class=MARS,\n",
    "        fit_function=fit_mars_model,\n",
    "        X_train=X_train.values,\n",
    "        y_train=y_train.values,\n",
    "        X_test=X_test.values,\n",
    "        y_test=y_test.values,\n",
    "        param_grid=param_grid,\n",
    "        n_jobs=multiprocessing.cpu_count() // 2,\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: max_terms cannot be larger than the dataset rows.\")\n",
    "    sys.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd19eb",
   "metadata": {},
   "source": [
    "### Grid Search and Model trained with CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1218c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.types.experimental.mars.mars_gpu import MARSGPU\n",
    "from models.selection.grid_search import GridSearch\n",
    "import cupy as cp\n",
    "\n",
    "param_grid = {\n",
    "    \"max_terms\": [400],\n",
    "    \"max_degree\": [2],\n",
    "    \"min_samples_split\": [4],\n",
    "    \"penalty\": [0.1]\n",
    "}\n",
    "\n",
    "def fit_mars_model_gpu(model_class: MARSGPU, X, y, max_terms, max_degree, min_samples_split, penalty):\n",
    "    X = cp.asarray(X)\n",
    "    y = cp.asarray(y)\n",
    "    model = model_class(\n",
    "        max_terms=max_terms,\n",
    "        max_degree=max_degree,\n",
    "        min_samples_split=min_samples_split,\n",
    "        penalty=penalty,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        model.fit(X, y)\n",
    "        print(f\"Fitted model with params: {max_terms}, {max_degree}, {min_samples_split}, {penalty}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model with params: {max_terms}, {max_degree}, {min_samples_split}, {penalty}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    return model\n",
    "\n",
    "grid_search = GridSearch()\n",
    "best_ensemble, grid_searches = grid_search.grid_search(\n",
    "    model_class=MARSGPU,\n",
    "    fit_function=fit_mars_model_gpu,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    param_grid=param_grid,\n",
    "    n_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6120f",
   "metadata": {},
   "source": [
    "# Best Estimator Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "78120490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "DriverAge:                                        0.003924337193548219\n",
      "ClaimDateDayCosine:                               0.003861007271874828\n",
      "ClaimDateDaySine:                                 0.001140381507788668\n",
      "SpecialJourneyExpenses:                           0.0009119794341765491\n",
      "InjuryDescription:                                0.0008035288476216096\n",
      "AccidentDateDayCosine:                            0.0006848504523391113\n",
      "PrognosisEndDateDayCosine:                        0.0005407252778467514\n",
      "SpecialTripCosts:                                 0.0004120669095982049\n",
      "AccidentType:                                     0.0003990609173478987\n",
      "PrognosisEndDateHourCosine:                       0.0003168048930721895\n",
      "AccidentDateHourCosine:                           9.003717832839486e-05\n",
      "SpecialEarningsLoss:                              3.8966901247647345e-05\n",
      "SpecialHealthExpenses:                            0.0\n",
      "SpecialReduction:                                 0.0\n",
      "SpecialAdditionalInjury:                          0.0\n",
      "SpecialMedications:                               0.0\n",
      "SpecialRehabilitation:                            0.0\n",
      "SpecialFixes:                                     0.0\n",
      "GeneralUplift:                                    0.0\n",
      "SpecialLoanerVehicle:                             0.0\n",
      "ExceptionalCircumstances:                         0.0\n",
      "MinorPsychologicalInjury:                         0.0\n",
      "DominantInjury:                                   0.0\n",
      "Whiplash:                                         0.0\n",
      "VehicleType:                                      0.0\n",
      "WeatherConditions:                                0.0\n",
      "NumberOfPassengers:                               0.0\n",
      "PoliceReportFiled:                                0.0\n",
      "WitnessPresent:                                   0.0\n",
      "Gender:                                           0.0\n",
      "AccidentDateYear:                                 0.0\n",
      "PrognosisEndDateMonthSine:                        -7.1114587313445e-05\n",
      "ClaimDateHourCosine:                              -0.00019844256169178343\n",
      "SpecialOverage:                                   -0.0002116633916453975\n",
      "SpecialUsageLoss:                                 -0.0002495069045048426\n",
      "PrognosisEndDateYear:                             -0.00028990166391201777\n",
      "ClaimDateMonthCosine:                             -0.000336748245471738\n",
      "AccidentDateHourSine:                             -0.0003533095998655561\n",
      "ClaimDateHourSine:                                -0.0006216682332361723\n",
      "AccidentClaimDeltaInDays:                         -0.0008438386433206879\n",
      "PrognosisEndDateHourSine:                         -0.0011638900888979037\n",
      "PrognosisEndDateDaySine:                          -0.0012962708317899307\n",
      "AccidentDateMonthSine:                            -0.0014893466359475888\n",
      "AccidentDateMonthCosine:                          -0.0014915593323094611\n",
      "PrognosisEndDateMonthCosine:                      -0.0015332750868920497\n",
      "AccidentDateDaySine:                              -0.0017956170049360232\n",
      "AccidentDescription:                              -0.0022855932958836395\n",
      "SpecialAssetDamage:                               -0.0027502456642638784\n",
      "ClaimDateYear:                                    -0.00339178759586678\n",
      "ClaimDateMonthSine:                               -0.003671064594168577\n",
      "VehicleAge:                                       -0.003708240976581914\n",
      "GeneralFixed:                                     -0.005661242206557449\n",
      "SpecialTherapy:                                   -0.006928761921251497\n",
      "InjuryPrognosisInDays:                            -0.012666435069220189\n",
      "GeneralRest:                                      -0.0797247598061085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def permutation_importance(model, X, y, metric):\n",
    "    \"\"\"\n",
    "    Custom permutation importance function for MARS models.\n",
    "\n",
    "    Parameters:\n",
    "    model (MARS): Trained MARS model.\n",
    "    X (np.ndarray): Feature matrix.\n",
    "    y (np.ndarray): Target vector.\n",
    "    metric (function): The metric used to evaluate the performance.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tuples (feature_index, importance_score).\n",
    "    \"\"\"\n",
    "    baseline_score = metric(y, model.predict(X))\n",
    "\n",
    "    importances = []\n",
    "    for col in range(X.shape[1]):\n",
    "        X_permuted = X.copy()\n",
    "        np.random.shuffle(X_permuted[:, col])\n",
    "        permuted_score = metric(y, model.predict(X_permuted))\n",
    "        importance = baseline_score - permuted_score\n",
    "        importances.append((col, importance))\n",
    "\n",
    "    return sorted(importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "feature_names = X_test.columns\n",
    "importance = permutation_importance(\n",
    "    best_ensemble[0],\n",
    "    X_test.values,\n",
    "    y_test.values,\n",
    "    mean_squared_error\n",
    ")\n",
    "\n",
    "# Replace feature indices with feature names\n",
    "importance = [(feature_names[idx], score) for idx, score in importance]\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Feature Importances:\")\n",
    "spacing = 50\n",
    "for feature_index, score in importance:\n",
    "    feature = f\"{feature_index}:\"\n",
    "    print(f\"{feature}{' ' * (spacing - len(feature))}{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721fd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021808873039950605\n",
      "0.02371639974399775\n"
     ]
    }
   ],
   "source": [
    "def ensemble_summary(self, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Generate a summary of the MARS ensemble's performance.\n",
    "\n",
    "    Parameters:\n",
    "    X_test (np.ndarray): Test feature matrix.\n",
    "    y_test (np.ndarray): Test target vector.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the ensemble's summary metrics.\n",
    "    \"\"\"\n",
    "    if not self.basis_:\n",
    "        raise ValueError(\"The model has not been trained yet.\")\n",
    "\n",
    "    # Aggregating results from all models in the ensemble\n",
    "    terms_used = []\n",
    "    gcv_values = []\n",
    "    r_squared_values = []\n",
    "    importance = {}\n",
    "\n",
    "    for model in self.models:\n",
    "        model_summary = model.summary(X_test, y_test)\n",
    "\n",
    "        # Aggregate terms used\n",
    "        terms_used.append(int(model_summary['selected_terms'].split(' ')[0]))\n",
    "\n",
    "        # Aggregate GCV and R²\n",
    "        gcv_values.append(model_summary[\"gcv\"])\n",
    "        r_squared_values.append(model_summary[\"r2\"])\n",
    "\n",
    "        # Aggregate importance (weighted by model contribution if applicable)\n",
    "        for predictor, imp in model_summary[\"importance\"].items():\n",
    "            if predictor not in importance:\n",
    "                importance[predictor] = []\n",
    "            importance[predictor].append(imp)\n",
    "\n",
    "    # Calculate ensemble-level metrics\n",
    "    avg_terms = np.mean(terms_used)\n",
    "    avg_gcv = np.mean(gcv_values)\n",
    "    avg_r2 = np.mean(r_squared_values)\n",
    "\n",
    "    # Average feature importance across all models\n",
    "    avg_importance = {\n",
    "        predictor: np.mean(importance[predictor]) for predictor in importance\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"ensemble_terms_used\": avg_terms,\n",
    "        \"ensemble_gcv\": avg_gcv,\n",
    "        \"ensemble_r2\": avg_r2,\n",
    "        \"ensemble_importance\": avg_importance,\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
